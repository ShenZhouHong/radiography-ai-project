{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime II Hyperparameter Search\n",
    "\n",
    "# Setup:\n",
    "\n",
    "## Connect to Remote Compute Environment\n",
    "\n",
    "First ensure we are connected to the correct VSCode Remote Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uname -nv && ls /"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upgrade Python Modules\n",
    "\n",
    "Install the latest version of Tensorflow, and install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --quiet --upgrade tensorflow==2.11.0\n",
    "!pip3 install --quiet tensorflow_addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Environment Checks\n",
    "\n",
    "Instantiate Python Kernel and load Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Attempt to dynamic GPU memory (vram) allocation\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(\n",
    "        tf.config.list_physical_devices('GPU')[0],\n",
    "        enable=True\n",
    "    )\n",
    "except IndexError as e:\n",
    "    print(\"No GPU detected. Dynamic GPU vRAM allocation failed.\")\n",
    "    \n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "from keras import layers\n",
    "from typing import Literal, Union, TypeVar\n",
    "\n",
    "# Import utility functions defined in ../common/ package\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from common import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-check GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tf.__version__)\n",
    "display(tf.config.list_physical_devices('GPU'))\n",
    "display(tf.test.gpu_device_name())\n",
    "\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(\n",
    "        tf.config.list_physical_devices('GPU')[0],\n",
    "        enable=True\n",
    "    )\n",
    "except IndexError as e:\n",
    "    display(\"No GPU Found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation\n",
    "\n",
    "Begin preparing the model's execution environment. First, we start by defining some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE  : tuple[int, int] = (299, 299)\n",
    "AUTOTUNE  : Literal = tf.data.AUTOTUNE\n",
    "RNG_SEED  : int = 1337\n",
    "\n",
    "# For Remote\n",
    "dataset_directory: str = \"./\"\n",
    "\n",
    "# For Local\n",
    "# dataset_directory: str = \"../../dataset/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n",
    "\n",
    "We load the datasets which are by default made available as `tf.data.Dataset` objects using a train-test-validation split of 70%, 15%, 15%. Since we are using k-fold validation, we must first concatenate the training and validation set, which will be split later on using our k-fold validation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 requires image tensors with a shape of (299, 299, 3) \n",
    "ds_train: tf.data.Dataset = tf.data.Dataset.load(dataset_directory + \"ds_train\")\n",
    "ds_valid: tf.data.Dataset = tf.data.Dataset.load(dataset_directory + \"ds_valid\")\n",
    "ds_test : tf.data.Dataset = tf.data.Dataset.load(dataset_directory + \"ds_test\")\n",
    "\n",
    "# For K-Fold Cross Validation\n",
    "ds_train_and_valid: tf.data.Dataset = ds_train.concatenate(ds_test)\n",
    "\n",
    "# Batching, caching, and performance optimisations are *not* performed at this stage\n",
    "# Since we are doing K-Fold validation\n",
    "\n",
    "# configure_for_performance(ds_train)\n",
    "# configure_for_performance(ds_valid)\n",
    "# configure_for_performance(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_dataset(ds_train_and_valid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regime II\n",
    "\n",
    "## Hyperparameters Under Consideration\n",
    "* Adam Optimizer:\n",
    "    * Learning Rate: 0.01 to 0.001 (default) to 0.0001\n",
    "        * 1.0e-1 to 1.0e-4\n",
    "        * [1, 2, 3, 4]\n",
    "    * epsilon: 0.00000001 to 0.1\n",
    "        * exponential search values:\n",
    "            * 1.0e-8 to 1.0e-1\n",
    "            * [1, 2, 3, 4, 5, 6, 7, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE: int = 1600\n",
    "DROPOUT_RATE: float = 0.2\n",
    "EPOCHS    : int = 20\n",
    "METRICS: list[any] = [\n",
    "    tf.keras.metrics.AUC(multi_label=True, num_labels=18),\n",
    "    tf.keras.metrics.Precision(thresholds=0.5),\n",
    "    tf.keras.metrics.Recall(thresholds=0.5),\n",
    "    tfa.metrics.F1Score(num_classes=18, average='macro', threshold=0.5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_gridsearch(\n",
    "        kfolds: int = 6,\n",
    "        filename: str = 'regime_II_search_results.pickle'\n",
    "    ) -> list[dict[str, Union[int, float, list[tf.keras.callbacks.History]]]]:\n",
    "    \"\"\"\n",
    "    Performs a grid search for hyperparameters 'learning_rate' and 'epsilon_rate' using K-Fold cross validation.\n",
    "\n",
    "    Args:\n",
    "        kfolds (int, optional): Number of folds for K-Fold cross validation. Defaults to 6.\n",
    "        filename (str, optional): File name to save the search results. Defaults to 'regime_II_search_results.pickle'.\n",
    "\n",
    "    Returns:\n",
    "        list[dict[str, Union[int, float, list[tf.keras.callbacks.History]]]]: List of dictionaries containing the search results.\n",
    "            Each dictionary contains the following keys:\n",
    "                - 'learning_rate' (float): The learning rate hyperparameter used in the experiment.\n",
    "                - 'epsilon_rate' (float): The epsilon rate hyperparameter used in the experiment.\n",
    "                - 'history_list' (list[tf.keras.callbacks.History]): List of Keras History objects containing training history for each fold in K-Fold cross validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the hyperparameter search grid\n",
    "    learning_rates: list = [1.0 * np.float_power(10, -rate) for rate in range(1, 5)]\n",
    "    epsilon_rates : list = [1.0 * np.float_power(10, -rate) for rate in range(1, 9)]\n",
    "\n",
    "    search_results: list[dict[str, Union[int, float, list[tf.keras.callbacks.History]]]] = []\n",
    "    for i, learning_rate in enumerate(learning_rates):\n",
    "        for j, epsilon_rate in enumerate(epsilon_rates):\n",
    "            index: int = (i * len(epsilon_rates)) + j\n",
    "            print(f\"\\n### Grid Search {index + 1}/{len(epsilon_rates) * len(learning_rates)}: learning_rate: {np.format_float_scientific(learning_rate)}, epsilon_rate: {np.format_float_scientific(epsilon_rate)} ###\")\n",
    "\n",
    "            # Conduct K-Fold Experiment\n",
    "            k_fold_results: list[tf.keras.callbacks.History] = cross_validate(\n",
    "                TransferLearningModel,\n",
    "                ds_train_and_valid,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                k=kfolds,\n",
    "                optimizer_kwargs={\"learning_rate\": learning_rate, \"epsilon\": epsilon_rate},\n",
    "                model_kwargs={\"dropout_rate\": DROPOUT_RATE}\n",
    "            )\n",
    "\n",
    "            search_results.append({\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"epsilon_rate\" : epsilon_rate,\n",
    "                \"history_list\" : k_fold_results\n",
    "            })\n",
    "\n",
    "            # Save results in case hyperparameter search gets interrupted\n",
    "            with open(filename, 'wb') as file:\n",
    "                pickle.dump(search_results, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print(\"ALL DONE\")\n",
    "    return search_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Search\n",
    "\n",
    "This will take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_gridsearch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
