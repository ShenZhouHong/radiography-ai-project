\chapter{Implementation and Analysis}\label{implementation}

In this chapter, we will present the implementation of the study methodology. Recall that the methodology has three components. We will begin with the establishment of an initial baseline, by creating and training a classical \enquote*{shallow} convolutional neural network based upon LeCun et al.'s 1998 LeNet model. \autocite{lenet1998} This classical CNN baseline will serve as the minimal performance standard that our model will aim to surpass. Next, utilising the InceptionV3 architecture which will serve as our transfer-learning base model, we will train an end-to-end (i.e.\ without transfer learning) model on our radiography dataset. This will serve as an additional baseline that will allow us to validate the transfer-learning \emph{technique} against regular end-to-end training.

Following the establishment of these two baselines, we will proceed to begin an initial evaluation of two different transfer-learning base models. We will compare the performance of InceptionV3 trained with ImageNet weights \autocite{imagenet}, against InceptionV3 trained with RadImageNet \autocite{radimagenet} weights. This initial evaluation will help us explore whether a base model trained on the smaller, but domain-specific RadImageNet dataset will have any advantages over the larger, but general ImageNet dataset. We will select the better performing base model out of the two options, and proceed to optimize the model's hyperparameters.

Our model's hyperparameter search procedure consists of two steps, which we term hyperparameter search Regime I and hyperparameter search Regime II. As per our methodology, in Regime I we find the optimal batch size and dropout rate for our model. This is done using a stochastic search process where the hyperparameter space of the model is randomly sampled for \(t\) trials, where each trial consists of a k-fold cross-validation of the model with the selected hyperparameters. Once the optimal combination of batch size and dropout rate are found, we will set these hyperparameters as constant and proceed to the second hyperparameter search regime. In Regime II we find the optimal learning rate and epsilon value \(\epsilon\) for the Adam optimizer, by conducting a grid search over a selection of possible values.

\section{K-Fold Evaluation}

Before we begin, we must first implement our k-fold cross-validation routine. Since model performance is sensitive to the network's random weight initialisation\footnote{This is particularly true on small datasets with unbalanced classes like ours.} \autocite{Narkhede2022}, our methodology requires k-fold cross-validation to be conducted on every experiment (i.e.\ model run). My implementation of the k-fold cross-validation process consists of two parts: a function which will divide the dataset into \(k\) folds, as well as a function that runs the k-fold cross-validation on the given model. The the \mintinline{python}{k_fold_dataset()} function is given as follows:\footnote{The code listings provided in this document \emph{are for illustration only}. The actual implementation is generally longer, and contains docstrings, debugging instrumentation, file I/O logic, as well as additional function arguments. Every listing will have a link to it's corresponding implementation in the git repository.}

\input{listings/kfold-dataset.tex}

\noindent
One thing of note, is that our \mintinline{python}{k_fold_dataset()} function conducts all dataset-related operations using the Tensorflow's high-performance \mintinline{python}{tf.data.Dataset} API. This allows support for pre-fetch, caching, and other low-level optimisations. This function serves as a dependency which is called by \mintinline{python}{cross_validate()}, which runs the actual K-fold cross validation experiments on the given model:

\input{listings/cross-validate.tex}

\noindent
The output of every k-fold cross-validation experiment will be a \enquote*{history list} containing \(k\) \mintinline{python}{tf.keras.callbacks.History} objects. This \mintinline{python}{History} object will contain training and validation metrics which will be used to calculate the average metric over \(k\) folds:

\input{listings/calc-mean-metrics.tex}


\noindent
The above code now completes the prerequisites necessary for data gathering.

\section{Establishing Baseline Performance Targets}

In this section, we will establish the baseline performance targets for our transfer-learning model by training and developing two models which will represent alternative approaches to the problem of multilabel classification on a small dataset. The baseline models will be: a \enquote*{shallow} CNN following LeCun et al.'s classical 1998 LeNet architecture \autocite{lenet1998}, and an InceptionV3 model that is directly end-to-end trained on our radiography dataset. We explicitly choose the above two models as our baseline for comparison, because they each help validate a different aspect of this project: whether a deep neural network is appropriate for the task in the first place, and whether the \emph{technique} of transfer learning is appropriate for our dataset. The second question of whether or not our technique is necessary is why we train a version of our model's architecture directly on the radiography data, in order to obtain a performance measure of using the same model architecture \emph{without} transfer learning. At minimum, our transfer-learning model must achieve a better performance (as measured by it's AUROC score) over the two baseline models.

The performance of the baseline models will be measured as the highest observed \emph{average} AUROC, found using k-fold cross-validation with \(k=10\). The value of \(k=10\) is chosen because the resulting per-fold training and validation splits are no larger than a conventional train, test, and validation split of 70\%, 15\%, 15\%, where:

\begin{itemize}
    \item Training and Validation Set (\texttt{ds\_train + ds\_valid}): 2490 (85\%):
    \begin{itemize}
        \item K-Fold Cross-Validation, $K$ = 10:
        \begin{itemize}
            \item Training Set: 2241 (\textasciitilde76\%)
            \item Validation Set: 249 (\textasciitilde8.5\% per fold)
        \end{itemize}
    \end{itemize}
    \item Hold-out Test Set (\texttt{ds\_test}): 441 (15\%)
\end{itemize}

\noindent
Larger \(k\) values yield a more thorough measurement of a model's performance at the cost of additional computational costs, while lower \(k\) values risk lowering the training-validation split ratio until the training set is too small for adequate training.

\subsection{Shallow Convolutional Neural Network}

\input{listings/lenet1998.tex}

\input{graphs/lenet1998.tex}

\subsection{End-to-End Training with InceptionV3}

\input{listings/inceptionv3.tex}

\input{graphs/inceptionv3-end2end.tex}

\subsection{Baseline Metrics}

\section{InceptionV3 with Transfer Learning}

\subsection{Base Model Trained on RadImageNet Dataset}

\input{graphs/inceptionv3-radimagenet.tex}

\subsection{Base Model Trained on InceptionV3 Dataset}

\input{graphs/inceptionv3-radimagenet.tex}

\subsection{Comparison between RadImageNet and ImageNet}

\section{Hyperparameter Search}

\subsection{Hyperparameter Search Regime I}

\input{listings/regime-1.tex}

\input{graphs/hypersearch-regime-1.tex}

\input{graphs/hypersearch-regime-1-examples.tex}

\input{graphs/regime-1-best-candidate.tex}

\subsection{Hyperparameter Search Regime II}

\input{listings/regime-2.tex}

\input{graphs/hypersearch-regime-2.tex}

\subsection{Final Hyperparameters}

\section{Final Model Performance}
