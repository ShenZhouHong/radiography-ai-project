\chapter{Implementation and Analysis}\label{implementation}

In this chapter, we will present the implementation of the study methodology. Recall that the methodology has three components. We will begin with the establishment of an initial baseline, by creating and training a classical \enquote*{shallow} convolutional neural network based upon LeCun et al.'s 1998 LeNet model. \autocite{lenet1998} This classical CNN baseline will serve as the minimal performance standard that our model will aim to surpass. Next, utilising the InceptionV3 architecture which will serve as our transfer-learning base model, we will train an end-to-end (i.e.\ without transfer learning) model on our radiography dataset. This will serve as an additional baseline that will allow us to validate the transfer-learning \emph{technique} against regular end-to-end training.

Following the establishment of these two baselines, we will proceed to begin an initial evaluation of two different transfer-learning base models. We will compare the performance of InceptionV3 trained with ImageNet weights \autocite{imagenet}, against InceptionV3 trained with RadImageNet \autocite{radimagenet} weights. This initial evaluation will help us explore whether a base model trained on the smaller, but domain-specific RadImageNet dataset will have any advantages over the larger, but general ImageNet dataset. We will select the better performing base model out of the two options, and proceed to optimize the model's hyperparameters.

Our model's hyperparameter search procedure consists of two steps, which we term hyperparameter search Regime I and hyperparameter search Regime II. As per our methodology, in Regime I we find the optimal batch size and dropout rate for our model. This is done using a stochastic search process where the hyperparameter space of the model is randomly sampled for \(t\) trials, where each trial consists of a k-fold cross-validation of the model with the selected hyperparameters. Once the optimal combination of batch size and dropout rate are found, we will set these hyperparameters as constant and proceed to the second hyperparameter search regime. In Regime II we find the optimal learning rate and epsilon value \(\epsilon\) for the Adam optimizer, by conducting a grid search over a selection of possible values.

\section{K-Fold Evaluation}

Before we begin, we must first implement our k-fold cross-validation routine. Since model performance is sensitive to the network's random weight initialisation\footnote{This is particularly true on small datasets with unbalanced classes like ours.} \autocite{Narkhede2022}, our methodology requires k-fold cross-validation to be conducted on every experiment (i.e.\ model run). My implementation of the k-fold cross-validation process consists of two parts: a function which will divide the dataset into \(k\) folds, as well as a function that runs the k-fold cross-validation on the given model. The the \mintinline{python}{k_fold_dataset()} function is given as follows:\footnote{The code listings provided in this document \emph{are for illustration only}. The actual implementation is generally longer, and contains docstrings, debugging instrumentation, file I/O logic, as well as additional function arguments. Every listing will have a link to it's corresponding implementation in the git repository.}

\input{listings/kfold-dataset.tex}

\noindent
One thing of note, is that our \mintinline{python}{k_fold_dataset()} function conducts all dataset-related operations using the Tensorflow's high-performance \mintinline{python}{tf.data.Dataset} API. This allows support for pre-fetch, caching, and other low-level optimisations. This function serves as a dependency which is called by \mintinline{python}{cross_validate()}, which runs the actual K-fold cross validation experiments on the given model:

\input{listings/cross-validate.tex}

\noindent
The output of every k-fold cross-validation experiment will be a \enquote*{history list} containing \(k\) \mintinline{python}{tf.keras.callbacks.History} objects. This \mintinline{python}{History} object will contain training and validation metrics which will be used to calculate the average metric over \(k\) folds:

\input{listings/calc-mean-metrics.tex}

\noindent
The above code now completes the prerequisites necessary for data gathering.

\section{Establishing Baseline Performance Targets}

In this section, we will establish the baseline performance targets for our transfer-learning model by training and developing two models which will represent alternative approaches to the problem of multilabel classification on a small dataset. The baseline models will be: a \enquote*{shallow} CNN following LeCun et al.'s classical 1998 LeNet architecture \autocite{lenet1998}, and an InceptionV3 model that is directly end-to-end trained on our radiography dataset. We explicitly choose the above two models as our baseline for comparison, because they each help validate a different aspect of this project: whether a deep neural network is appropriate for the task in the first place, and whether the \emph{technique} of transfer learning is appropriate for our dataset. The second question of whether or not our technique is necessary is why we train a version of our model's architecture directly on the radiography data, in order to obtain a performance measure of using the same model architecture \emph{without} transfer learning. At minimum, our transfer-learning model must achieve a better performance (as measured by it's AUROC score) over the two baseline models.

The performance of the baseline models will be measured as the highest observed \emph{average} AUROC, found using k-fold cross-validation with \(k=10\). The value of \(k=10\) is chosen because the resulting per-fold training and validation splits are no larger than a conventional train, test, and validation split of 70\%, 15\%, 15\%, where:

\begin{itemize}
    \item Training and Validation Set (\texttt{ds\_train + ds\_valid}): 2490 (85\%):
    \begin{itemize}
        \item K-Fold Cross-Validation, $K$ = 10:
        \begin{itemize}
            \item Training Set:  2241  (\textasciitilde76\%)
            \item Validation Set: 249  (\textasciitilde8.5\% per fold)
        \end{itemize}
    \end{itemize}
    \item Hold-out Test Set (\texttt{ds\_test}): 441 (15\%)
\end{itemize}

\noindent
Larger \(k\) values yield a more thorough measurement of a model's performance at the cost of additional computational costs, while lower \(k\) values risk lowering the training-validation split ratio until the training set is too small for adequate training. For this initial evaluation, as we wish to yield a benchmark for baseline performance, we will be using a \(k\) value of 10. For the hyperparameter search regime, we will be using \(k=6\) in order to lower computational costs.

\subsection{Shallow Convolutional Neural Network}

For the first benchmark, we begin by implementing the shallow convolutional neural network described by LeCun et al in \autocite{lenet1998} in Tensorflow. Our implementation follows the original paper, with a slight modification in the final classifier, in order to output the 18-vector one-hot encoded label predictions. Note the presence of only two convolutional layers --- this is typical for early CNNs of that period.

\input{listings/lenet1998.tex}

\noindent
We implement our version of the LeNet architecture by subclassing \mintinline{python}{tf.keras.Model} class, which is then passed on to our \mintinline{python}{cross_validate()} function to be evaluated. This entire experiment is conducted within a Jupyter notebook which is made available as a self-contained, reproducible unit within the project repository (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/master/python/initial-evaluation/lenet1998.ipynb}{Github}). Running the experiment yields our first AUROC to performance graph:

\input{graphs/lenet1998.tex}

\noindent 
The bold lines in the chart represent the \emph{average} training (red) and validation (blue) AUROC, as measured after performing k-fold cross-validation on 10 folds (\(k = 10\)). The transparent lines indicate the observed training and validation AUROC per each individual fold: this per-fold performance has been charted in order for us to better observe the consistency of model performance per epoch. Variations in performance per fold is due to a combination of different random starting conditions (due to random weight initialisation at the start of a model's training), as well as variances in floating-point calculations.

What information does our data for the LeNet model tell us? First, we can observe severe overfitting: by epoch 10, performance on the training set asymptomatically approaches $1.0$ (as quantified by AUROC). However, the validation performance remains minimal: generally averaging around $0.60$, with certain instances of the model performing little better than chance ($0.50$). This information indicates that a classical \enquote*{shallow} CNN lacks the representational power to extract the features necessary to perform classification on our dataset. Indeed, it appears that the LeNet model fails to converge at all. This is to be expected: and our experiment yields a minimal baseline AUROC value of $0.608$ that our subsequent models must beat. Likewise, by demonstrating that classical \enquote*{shallow} CNNs are unable to solve our problem, we make the case for using a \enquote*{deep} neural network: in the form of the InceptionV3 architecture, which we will explore in the following section.

\subsection{End-to-End Training with InceptionV3}

Having validated the necessity of using a deep convolutional neural network to solve this \emph{multiclass}, \emph{multilabel} classification task, our next question would be: \enquote{is it necessary to use the technique of \emph{transfer-learning} on our dataset, or would a regular end-to-end training process suffice?} Although the small size of our dataset indicates that transfer learning is appropriate, it is important for us to validate our assumptions through empirical data. Hence, we arrive at the establishment of the second baseline model: end-to-end training InceptionV3 on our dataset. Let us start by defining our base model:

\input{listings/inceptionv3.tex}

\noindent
We define a \mintinline{python}{class TransferLearningModel} which will be instantiated by every k-fold validation trial. Note that for this particular experiment, as we are establishing an end-to-end trained baseline, we will be setting \mintinline{Python}{self.inceptionv3(weights=None)} and the attribute \mintinline{python}{self.inceptionv3.trainable = True}. Naturally in the actual implementation (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/cf8c9e9a1f07849787a98b2fc51df690354bf194/python/common/model.py}{Github}) this is done through an argument in the class constructor, however the listing is simplified for the purpose of size and readability. So what happens now when we run the kfold experiment (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/ef29e4cb63fc38185b3bb45fc37027df3e385a44/python/initial-evaluation/inceptionv3-end2end.ipynb}{Github})?

\input{graphs/inceptionv3-end2end.tex}

\noindent
We can observe that the end-to-end variant performs marginally: achieving an average AUROC of \(0.692\). However, upon a closer examination it is clear that the validation AUROC of each individual k-fold trial is highly erratic. The large spikes in validation AUROC indicates a failure to converge, as the dataset is too small for the number of tunable weights in the model. The InceptionV3 model has 189 layers, with a combined total of 23.9 million trainable weights: representing a parameter space several orders of magnitude larger than our dataset. The highly erratic validation AUROC is only a symptom of the model's inability to converge, and demonstrates clearly that regular end-to-end training is insufficient and inappropriate for our dataset.

\subsection{Baseline Metrics}

Having completed assessing our two baseline models, we are left with the following metrics that will help us in our own evaluation:

\input{tables/baseline-benchmarks.tex}

\noindent
The first level of performance that our subsequent models are expected to achieve is a validation AUROC \(> 0.50\). As a measurement of classifier performance, an AUROC of \(0.50\) indicates performance no better than chance (i.e. the same as choosing by random). If we are unable to meet the minimum baseline of \(> 0.50\), then our entire approach may be unrealistic and infeasible. The second baseline that we must achieve is a performance of \(> 0.61\). For that is the best performance measured from a classical \enquote*{shallow} CNN. As deep neural networks, with their dozens (if not hundreds) of layers incur a computational cost that is an order of magnitude above classical \enquote*{shallow} CNNs, if our model is not able to exceed the performance of a regular CNN, it will be better to develop a regular CNN instead. Finally, the last baseline that we established allows us to validate the suitability of the transfer-learning technique. If our model is unable to meet an AUROC of \(> 0.69\), then we will be better served to train our model architecture directly on our dataset.

With this information in hand, it is time for us to embark on the second part of our study: developing a transfer-learning model to infer the RUST score of radiographs of long-bone fractures. In the transfer learning technique, a base model is trained on a larger dataset, before having it's weights frozen, and then used as a component of a classifier trained on the task-specific dataset. A key decision in this process is the choice of the larger dataset that the base model will be trained on. Thus, we are lead to the second part of our study: evaluating the performance of InceptionV3 trained on ImageNet, and RadImageNet.

\section{InceptionV3 with Transfer Learning}

Let us begin by evaluating the performance of the RadImageNet weights. Following the same procedure as we did earlier, we instantiate a InceptionV3 model with our classifier, and set the attribute \mintinline{Python}{self.inceptionv3(weights='radimagenet.h5')} and \mintinline{python}{self.inceptionv3.trainable = False}. This class instance now has InceptionV3 with pre-trained ImageNet weights, which we have frozen. Now when we run the training process, only our classifier will be trained. We conduct the experiment in the following Jupyter notebook (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/8d295305fbf9e8a7d1993e4564731e3f3f113f2d/python/initial-evaluation/inceptionv3-radimgnet.ipynb}{Github}).

\subsection{Base Model Trained on RadImageNet Dataset}

The model trained on RadImageNet weights achieves an average validation AUROC of \(0.706\). Note how unlike the version of InceptionV3 that was end-to-end trained, the per-fold validation AUROC is fairly consistent: we do not see any large spikes in validation performance. Likewise, although the model begins to exhibit overfitting after epoch 15, the degree of overfitting is relatively well controlled.

\input{graphs/inceptionv3-radimagenet.tex}

\noindent
This preliminary information helps inform us that the technique of transfer learning is appropriate for our use case and dataset. All that follows now is for us to evaluate the ImageNet weights, and compare their performances together.

\subsection{Base Model Trained on InceptionV3 Dataset}

We now conduct the same experiment in a separate Jupyter notebook with ImageNet weights (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/8d295305fbf9e8a7d1993e4564731e3f3f113f2d/python/initial-evaluation/inceptionv3-imagenet.ipynb}{Github}). Recall that the difference between RadImagenet and ImageNet is that former contains approximately 4.1 million images \autocite{radimagenet}, while the latter contains around 15.0 million \autocite{imagenet}. While the RadImageNet dataset is exclusively sourced from medical imagery, including radiographs --- the vastly larger ImageNet dataset has the potential to perform better, simply because the model was trained on a larger dataset. Does this assumption hold true?

\input{graphs/inceptionv3-imagenet.tex}

\noindent
Our data indicates that the InceptionV3 weights yield a higher validation AUROC of \(0.784\), in comparison to the RadImageNet model with an AUROC of \(0.706\). This means that although the RadImageNet dataset was more domain-specific to our needs (i.e. medical radiography classification), it appears the sheer size of the ImageNet datset yielded weights which performed better.

\subsection{Comparison between RadImageNet and ImageNet}

However, despite this difference in \enquote*{naive} (i.e. untuned, without hyperparameter optimisation), the performance characteristics of both models are quite different. Observe how the overfitting profile of the RadImageNet model is less severe than that of ImageNet, despite achieving a lower validation AUROC overall. Likewise, while the validation AUROC of the ImageNet model begins to decrease past epoch 15 due to overfitting, the validation AUROC of RadImageNet is still growing by epoch 50. Although our methodology calls for us to select the better performing model out of both of them, the information shown here offers room for further investigation --- which can be the subject of a future study.

At this point, having assessed the \enquote*{naive} performance of both the RadImageNet weights and the regular ImageNet weights, we will select the better-performing ImageNet weights as the basis for our transfer learning model. Going forward, we are now ready to tackle our problem directly: and begin the process of hyperparameter search.

\section{Hyperparameter Search}

Recall that our methodology calls for a two-part hyperparameter search (see \autoref{sec:hypersearch}), consisting of Hyperparameter Search Regime I, and Hyperparameter Search Regime II. Regime I is concerned with finding the best combination of batch size and dropout, while Regime II is for the best learning rate and epsilon \(\epsilon\) for the Adam optimizer.

\subsection{Hyperparameter Search Regime I}

To begin, let us construct a search function which conducts \(t\) trials, where during each trial a random portion of the hypothesis space is sampled, and then evaluated using k-fold validation with \(k = 6\). Recall that our methodology specifies the use of a slightly lower \(k\) value as a concession to the amount of compute resources available. Given \(20\) epochs of training per k-fold, and \(6\) folds per trial, this hyperparameter search regime conducts a total of 12,000 epochs of training over the course of 2 days (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/master/python/hyperparam-search/regime-1.ipynb}{Github}).

\input{listings/regime-1.tex}

\noindent
The results of this hyperparameter search are then plotted, with the maximum observed validation AUROC represented as the colour of the data point in the following scatter plot. Note that the color-map is scaled according to the highest and lowest observed average validation AUROC.

\input{graphs/hypersearch-regime-1.tex}

\noindent
What were some of the learning rate and batch size combinations that we found? The following table lists out the top ten hyperparameter options.

\input{tables/regime-1-top-ten.tex}

\noindent
As we can from a glance, a reasonable batch size seems to hover between \(1500\) to \(2000\). Likewise, a good dropout value seems to be around \(0.20\). This corresponds the colour gradient that we can see in the scatter plot. As an additional visualisation, we will also graph a random assortment of nine hyperparameter choices from our hypothesis space sample:

\input{graphs/hypersearch-regime-1-examples.tex}

\noindent
The graphs above represent a selection of nine batch size and dropout combinations, sorted by best-performing to worse-performing. The first graph represents the top-performing hyperparameter combination, which we will graphs in greater detail below:

\input{graphs/regime-1-best-candidate.tex}

\noindent
Thus, we complete the first hyperparameter search regime, obtaining the hyperparameter options of \(1599\) for batch size and \(0.205\) for dropout. Going forward in all subsequent models, we will use those values, after rounding them up slightly to the nearest whole figure: \mintinline{python}{batch_size: int = 1600, dropout_rate: float = 0.20}. With this information, we may now move on to the second hyperparameter search regime, where we find the best hyperparameters for the Adam optimizer.

\subsection{Hyperparameter Search Regime II}

Recall that for Regime II, we must find the best learning rate and epsilon factor for the Adam optimizer. We begin by implementing a grid search function, as seen below. We sample the grid at discrete intervals for both learning rate and epsilon:

\input{listings/regime-2.tex}

\noindent
Running the above search routine in a Jupyter notebook (\href{https://github.com/ShenZhouHong/radiography-ai-project/blob/13893a1c14cbfdd78876fda2a45aa765377d7cfc/python/hyperparam-search/regime-2.ipynb}{Github}) on our hosted compute provider, we obtain the following results:

\input{graphs/hypersearch-regime-2.tex}

\input{tables/regime-2-top-ten.tex}

\subsection{Final Hyperparameters}

Thus, we have completed our hyperparameter search. After implementing and running Regime I and Regime II (\href{https://github.com/ShenZhouHong/radiography-ai-project/tree/13893a1c14cbfdd78876fda2a45aa765377d7cfc/python/hyperparam-search}{Github}), we were able to find the following hyperparameter options for our final model:

\input{tables/final-hyperparameters.tex}

\noindent
The implementation, analysis, and search of our model hyperparameters are now complete. Now we are ready for the final evaluation of the complete model on our hold-out test set. This will be done in the final part of our project, in the Part IV evaluation.